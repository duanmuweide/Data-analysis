# 虚拟机操作综合指南

本指南提供在虚拟机环境中完成大数据分析项目的完整流程，不使用Python，包含从环境准备到结果可视化的所有步骤。

## 一、环境准备

### 1. 检查Hadoop、Hive、HBase环境

```bash
# 检查Hadoop状态
hdfs dfsadmin -report

# 检查Hive版本
hive --version

# 检查HBase版本
hbase version

# 检查Zookeeper状态
zkServer.sh status
```

### 2. 启动服务（如未启动）

```bash
# 启动Hadoop
sbin/start-all.sh

# 启动Zookeeper
zkServer.sh start

# 启动HBase
bin/start-hbase.sh
```

## 二、HBase表创建与数据导入

### 1. 创建HBase表

```bash
# 执行HBase表创建脚本
hbase shell hbase表创建.txt
```

### 2. 数据导入方法（3种可选）

#### 方法1：HBase Import工具导入

```bash
# 1.1 准备数据（在Hive中）
hive -e "
CREATE TABLE crime_hbase_import (
    rowkey STRING,
    incident_incident_id STRING,
    incident_offence_code STRING,
    incident_cr_number STRING,
    incident_nibrs_code STRING,
    incident_victims INT,
    incident_crime_name1 STRING,
    incident_crime_name2 STRING,
    incident_crime_name3 STRING,
    time_dispatch_time STRING,
    time_start_time STRING,
    time_end_time STRING,
    location_block_address STRING,
    location_city STRING,
    location_state STRING,
    location_zip_code STRING,
    location_latitude STRING,
    location_longitude STRING,
    location_address_num STRING,
    location_street_prefix STRING,
    location_street_name STRING,
    location_street_suffix STRING,
    location_street_type STRING,
    police_police_district STRING,
    police_agency STRING,
    police_place STRING,
    police_sector STRING,
    police_beat STRING,
    police_pra STRING,
    police_district_num STRING
) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

INSERT INTO TABLE crime_hbase_import
SELECT 
    CONCAT(city, ':', DATE_FORMAT(dispatch_time, 'yyyy-MM-dd'), ':', incident_id) as rowkey,
    incident_id,
    offence_code,
    cr_number,
    nibrs_code,
    victims,
    crime_name1,
    crime_name2,
    crime_name3,
    DATE_FORMAT(dispatch_time, 'yyyy-MM-dd HH:mm:ss'),
    DATE_FORMAT(start_time, 'yyyy-MM-dd HH:mm:ss'),
    DATE_FORMAT(end_time, 'yyyy-MM-dd HH:mm:ss'),
    block_address,
    city,
    state,
    zip_code,
    CAST(latitude AS STRING),
    CAST(longitude AS STRING),
    address_num,
    street_prefix,
    street_name,
    street_suffix,
    street_type,
    police_district,
    agency,
    place,
    sector,
    beat,
    pra,
    district_num
FROM crime_incidents;

INSERT OVERWRITE LOCAL DIRECTORY '/tmp/crime_hbase_data'
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t'
SELECT * FROM crime_hbase_import;
"

# 1.2 上传文件到HDFS
hdfs dfs -put /tmp/crime_hbase_data /user/hadoop/

# 1.3 使用Import工具导入
hbase org.apache.hadoop.hbase.mapreduce.Import crime_data_hbase /user/hadoop/crime_hbase_data
```

#### 方法2：Hive-HBase集成导入

```bash
# 执行Hive-HBase集成脚本
hive -e "
ADD JAR /path/to/hbase-hive-handler.jar;
ADD JAR /path/to/hbase-client.jar;
ADD JAR /path/to/hbase-common.jar;
ADD JAR /path/to/hbase-server.jar;
ADD JAR /path/to/zookeeper.jar;

CREATE EXTERNAL TABLE crime_hbase_hive (
    rowkey STRING,
    incident_id STRING,
    offence_code STRING,
    cr_number STRING,
    nibrs_code STRING,
    victims INT,
    crime_name1 STRING,
    crime_name2 STRING,
    crime_name3 STRING,
    dispatch_time STRING,
    start_time STRING,
    end_time STRING,
    block_address STRING,
    city STRING,
    state STRING,
    zip_code STRING,
    latitude STRING,
    longitude STRING,
    address_num STRING,
    street_prefix STRING,
    street_name STRING,
    street_suffix STRING,
    street_type STRING,
    police_district STRING,
    agency STRING,
    place STRING,
    sector STRING,
    beat STRING,
    pra STRING,
    district_num STRING
) STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
WITH SERDEPROPERTIES (
    "hbase.columns.mapping" = ":key,incident:incident_id,incident:offence_code,incident:cr_number,incident:nibrs_code,incident:victims,incident:crime_name1,incident:crime_name2,incident:crime_name3,time:dispatch_time,time:start_time,time:end_time,location:block_address,location:city,location:state,location:zip_code,location:latitude,location:longitude,location:address_num,location:street_prefix,location:street_name,location:street_suffix,location:street_type,police:police_district,police:agency,police:place,police:sector,police:beat,police:pra,police:district_num"
)
TBLPROPERTIES (
    "hbase.table.name" = "crime_data_hbase"
);

INSERT INTO TABLE crime_hbase_hive
SELECT 
    CONCAT(city, ':', DATE_FORMAT(dispatch_time, 'yyyy-MM-dd'), ':', incident_id) as rowkey,
    incident_id,
    offence_code,
    cr_number,
    nibrs_code,
    victims,
    crime_name1,
    crime_name2,
    crime_name3,
    DATE_FORMAT(dispatch_time, 'yyyy-MM-dd HH:mm:ss'),
    DATE_FORMAT(start_time, 'yyyy-MM-dd HH:mm:ss'),
    DATE_FORMAT(end_time, 'yyyy-MM-dd HH:mm:ss'),
    block_address,
    city,
    state,
    zip_code,
    CAST(latitude AS STRING),
    CAST(longitude AS STRING),
    address_num,
    street_prefix,
    street_name,
    street_suffix,
    street_type,
    police_district,
    agency,
    place,
    sector,
    beat,
    pra,
    district_num
FROM crime_incidents;
"
```

#### 方法3：HBase Shell批量导入

```bash
# 准备数据文件，格式：rowkey column_family:column value
# 示例：BALTIMORE:2023-12-09:1234567 incident:incident_id 1234567

# 执行批量导入
hbase shell << EOF
import org.apache.hadoop.hbase.client.Put
import org.apache.hadoop.hbase.util.Bytes
import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.ConnectionFactory
import org.apache.hadoop.hbase.client.Table
import org.apache.hadoop.hbase.TableName
import org.apache.hadoop.hbase.client.BufferedMutator
import org.apache.hadoop.hbase.client.BufferedMutatorParams

val conf = HBaseConfiguration.create()
val connection = ConnectionFactory.createConnection(conf)
val tableName = TableName.valueOf("crime_data_hbase")
val params = new BufferedMutatorParams(tableName)
params.writeBufferSize(1024*1024*1024) // 1GB
val mutator = connection.getBufferedMutator(params)

val file = new java.io.File("/tmp/crime_hbase_batch.txt")
val lines = scala.io.Source.fromFile(file).getLines()

lines.foreach { line =>
  val parts = line.split("\t")
  if (parts.length == 3) {
    val rowkey = parts(0)
    val column = parts(1)
    val value = parts(2)
    val cf_col = column.split(":")
    if (cf_col.length == 2) {
      val cf = cf_col(0)
      val col = cf_col(1)
      val put = new Put(Bytes.toBytes(rowkey))
      put.addColumn(Bytes.toBytes(cf), Bytes.toBytes(col), Bytes.toBytes(value))
      mutator.mutate(put)
    }
  }
}

mutator.flush()
mutator.close()
connection.close()
EOF
```

### 3. 验证HBase数据导入

```bash
hbase shell << EOF
# 验证表是否存在
list

# 查看表结构
describe 'crime_data_hbase'

# 扫描表数据
scan 'crime_data_hbase', {LIMIT => 10}

# 按RowKey查询
get 'crime_data_hbase', 'BALTIMORE:2023-12-09:1234567'

# 按城市查询
scan 'crime_data_hbase', {ROWPREFIXFILTER => 'BALTIMORE:', LIMIT => 10}
EOF
```

## 三、Hive表创建与数据导入

### 1. 创建Hive表

```bash
# 执行Hive表创建脚本
hive -f 创建Hive外部表/hive.sql
```

### 2. 导入数据到Hive

```bash
# 执行数据导入脚本
hive -f 创建Hive外部表/数据导入.sql
```

## 四、Hive统计分析

### 执行统计分析SQL

```bash
# 执行所有统计分析
hive -f hive统计分析.sql
```

## 五、自定义Hive函数使用

### 1. 编译并打包自定义函数

```bash
# 编译Java文件
javac -cp $(hive -e "set hive.home")/lib/* CrimeSeverityUDF.java

# 打包成JAR
jar cvf CrimeSeverityUDF.jar CrimeSeverityUDF.class
```

### 2. 在Hive中使用自定义函数

```bash
hive -e "
# 添加JAR
hive> ADD JAR CrimeSeverityUDF.jar;

# 创建临时函数
hive> CREATE TEMPORARY FUNCTION get_crime_severity AS 'CrimeSeverityUDF';

# 使用自定义函数进行分析
SELECT 
    crime_name1,
    get_crime_severity(crime_name1) as severity_level,
    COUNT(*) as crime_count
FROM crime_incidents
GROUP BY crime_name1
ORDER BY crime_count DESC;
"
```

## 六、Hive结果导出到MySQL

### 方法1：使用Sqoop导出

```bash
# 导出示例

# 1. 创建MySQL表
mysql -u root -p -e "
CREATE DATABASE IF NOT EXISTS crime_analysis;
USE crime_analysis;
CREATE TABLE IF NOT EXISTS crime_by_city (
    city VARCHAR(50),
    crime_name1 VARCHAR(100),
    crime_count INT,
    avg_victims DOUBLE,
    PRIMARY KEY (city, crime_name1)
);
"

# 2. 使用Sqoop导出
sqoop export \
    --connect jdbc:mysql://localhost:3306/crime_analysis \
    --username root \
    --password root \
    --table crime_by_city \
    --export-dir /user/hive/warehouse/crime_by_city \
    --input-fields-terminated-by '\t' \
    --input-lines-terminated-by '\n' \
    --m 1
```

### 方法2：使用Hive-JDBC导出

```bash
# 示例：使用beeline导出

# 1. 创建导出表
hive -e "
CREATE TABLE crime_by_city_export (
    city STRING,
    crime_name1 STRING,
    crime_count INT,
    avg_victims DOUBLE
) STORED AS TEXTFILE;

INSERT INTO TABLE crime_by_city_export
SELECT city, crime_name1, COUNT(*) as crime_count, AVG(victims) as avg_victims
FROM crime_incidents
GROUP BY city, crime_name1;
"

# 2. 使用beeline导出到MySQL
beeline -u jdbc:hive2://localhost:10000 -n hadoop -p hadoop -e "
INSERT OVERWRITE TABLE crime_analysis.crime_by_city
SELECT * FROM crime_by_city_export;
"
```

## 七、数据可视化

### 1. 使用JeeCG Report进行可视化

```bash
# 1. 启动JeeCG Report服务
sh /opt/jeecg/bin/start.sh

# 2. 访问JeeCG Report控制台
# 浏览器访问：http://localhost:8080/jeecg-report

# 3. 配置数据源
# - 类型：MySQL
# - 名称：crime_analysis
# - 连接字符串：jdbc:mysql://localhost:3306/crime_analysis
# - 用户名：root
# - 密码：root

# 4. 创建报表
# - 选择数据源：crime_analysis
# - 选择表：crime_by_city
# - 设计图表类型：柱状图、折线图等
# - 设置参数和样式

# 5. 发布报表
# - 保存报表
# - 生成报表URL
```

### 2. 使用Finereport进行可视化

```bash
# 1. 启动Finereport服务
sh /opt/finereport/bin/start.sh

# 2. 访问Finereport控制台
# 浏览器访问：http://localhost:8075/WebReport/ReportServer

# 3. 配置数据源
# - 新建数据源：MySQL
# - 名称：crime_analysis
# - 连接信息：
#   - 驱动：com.mysql.jdbc.Driver
#   - URL：jdbc:mysql://localhost:3306/crime_analysis
#   - 用户名：root
#   - 密码：root

# 4. 创建模板
# - 新建模板：决策报表
# - 拖拽数据集：crime_by_city
# - 设计图表：柱状图、折线图、饼图等
# - 设置过滤条件和样式

# 5. 预览和发布
# - 预览报表
# - 保存并发布到服务器
```

## 八、项目进度管理

### 1. 更新项目进度

```bash
# 使用项目进度统计模板记录进度
# 模板文件：项目进度统计模板.md

# 每周五18:00前更新进度
# 包含：
# - 本周完成的任务
# - 下周计划完成的任务
# - 遇到的问题和解决方案
# - 风险评估
```

### 2. 检查项目状态

```bash
# 定期检查各组件状态
sbin/start-all.sh status
zkServer.sh status
bin/start-hbase.sh status

# 检查数据质量
# 示例：检查Hive表数据量
hive -e "SELECT COUNT(*) FROM crime_incidents;"

# 检查HBase表数据量
hbase shell -e "count 'crime_data_hbase'"
```

## 九、常见问题及解决方案

### 1. HDFS权限问题

```bash
# 查看文件权限
hdfs dfs -ls /user/hive/warehouse/

# 修改文件权限
hdfs dfs -chmod -R 777 /user/hive/warehouse/
```

### 2. Hive-HBase集成问题

```bash
# 检查JAR包是否存在
ls $(hive -e "set hive.home")/lib/*hbase*

# 确保HBase服务正在运行
bin/start-hbase.sh status

# 检查Zookeeper连接
hbase zkcli ls /hbase
```

### 3. Sqoop导出问题

```bash
# 检查MySQL连接
telnet localhost 3306

# 检查Sqoop配置
cat $(sqoop version | grep "Sqoop home" | awk '{print $3}')/conf/sqoop-site.xml

# 查看Sqoop日志
cat /tmp/hadoop/sqoop-*.log
```

### 4. 性能优化

```bash
# 调整Hive并行度
hive -e "SET hive.exec.parallel=true; SET hive.exec.parallel.thread.number=8;"

# 调整HBase写入缓冲区
hbase shell -e "alter 'crime_data_hbase', {NAME => 'incident', BLOCKSIZE => 65536, WRITE_BUFFER_SIZE => 134217728}"

# 增加Hadoop YARN资源
sed -i 's/<name>yarn.nodemanager.resource.memory-mb<\/name>/<name>yarn.nodemanager.resource.memory-mb<\/name><value>8192<\/value>/' /usr/local/hadoop/etc/hadoop/yarn-site.xml
```

## 十、关机与资源清理

### 1. 停止服务

```bash
# 停止HBase
bin/stop-hbase.sh

# 停止Zookeeper
zkServer.sh stop

# 停止Hadoop
sbin/stop-all.sh
```

### 2. 清理临时文件

```bash
# 清理HDFS临时文件
hdfs dfs -rm -r /tmp/*

# 清理本地临时文件
rm -rf /tmp/crime_*

# 清理日志文件
rm -rf /usr/local/hadoop/logs/*
rm -rf /usr/local/hive/logs/*
rm -rf /usr/local/hbase/logs/*
```

---

本指南提供了完整的虚拟机操作流程，涵盖了从环境准备到结果可视化的所有步骤。按照指南执行，可以确保顺利完成大数据分析项目的所有任务。